{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to query the STAC API and return an xarray. The query returns a large number of images within a time window, and these are used to create a GIF which shows activity at a mining site (LEFR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "import shapely.geometry\n",
    "from IPython.display import display\n",
    "from pystac_client import Client\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "from glob import glob\n",
    "from odc.stac import configure_rio, stac_load\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dask cluster, just run once per session\n",
    "client = dask.distributed.Client()\n",
    "configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n",
    "display(client)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with the STAC API, if you get an error check your credentials in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "def get_new_token():\n",
    "    auth_server_url = os.getenv(\"EDS_AUTH_URL\")\n",
    "    client_id = os.getenv(\"EDS_CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"EDS_SECRET\")\n",
    "    token_req_payload = {'grant_type': 'client_credentials'}\n",
    "\n",
    "    token_response = requests.post(\n",
    "        auth_server_url,\n",
    "        data=token_req_payload,\n",
    "        verify=False,\n",
    "        allow_redirects=False,\n",
    "        auth=(client_id, client_secret)\n",
    "    )\n",
    "    token_response.raise_for_status()\n",
    "\n",
    "    tokens = json.loads(token_response.text)\n",
    "    return tokens['access_token']\n",
    "\n",
    "token = get_new_token()\n",
    "\n",
    "catalog = Client.open(os.getenv(\"EDS_API_URL\"), headers={\n",
    "    \"Authorization\": f\"bearer {token}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now select mission & catalogue\n",
    "Edit the cell below to select between \"S2\" or \"VENUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SATELLITE = \"VENUS\" # \"S2\" or \"VENUS\"\n",
    "\n",
    "if SATELLITE == \"S2\":\n",
    "    COLLECTION = \"sentinel-s2-l2a\"\n",
    "    BANDS = (\"B04\", \"B03\", \"B02\")\n",
    "elif SATELLITE == \"VENUS\":\n",
    "    catalog = catalog\n",
    "    COLLECTION = \"venus-l2a\"\n",
    "    BANDS = (\"red\", \"green\", \"blue\", ) # \"nir08\", \"rededge\", \"yellow\", \"coastal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup location to save generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE = \"LEFR3\" # site name in this tutorial\n",
    "\n",
    "OUTPUTS_DIR = f\"outputs_{SITE}_{SATELLITE}/\" # location where the output files will be saved\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.makedirs(OUTPUTS_DIR)\n",
    "\n",
    "EMPTY_OUTPUTS_DIR = True # set to True to delete all files in OUTPUTS_DIR before running the notebook\n",
    "if EMPTY_OUTPUTS_DIR:\n",
    "    for f in glob(OUTPUTS_DIR + \"*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "print(f\"Using {SATELLITE} data from {catalog.description}\")\n",
    "print(f\"Writing outputs to {OUTPUTS_DIR}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leafmap\n",
    "This method requires having the URL for one of the COGS - get the url for the B1 band from the EDA catalogue\n",
    "\n",
    "- Not displaying for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "Map = leafmap.Map()\n",
    "image_file_FRE_B1 = \"s3://venus-l2a-cogs/LEFR3/2020/10/08/VENUS-XS_20201008-020631-000_L2A_LEFR3_D/VENUS-XS_20201008-020631-000_L2A_LEFR3_C_V3-0_FRE_B1.tif\"\n",
    "\n",
    "Map.add_cog_layer(image_file_FRE_B1, name=SITE)\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = Map.user_roi\n",
    "if not ROI:\n",
    "    print(\"Draw a polygon using the rectangle tool, defaulting\")\n",
    "    ROI = {'type': 'Feature',\n",
    " 'properties': {},\n",
    " 'geometry': {'type': 'Polygon',\n",
    "  'coordinates': [[[121.735811, -31.309868],\n",
    "    [121.735811, -31.293184],\n",
    "    [121.763749, -31.293184],\n",
    "    [121.763749, -31.309868],\n",
    "    [121.735811, -31.309868]]]}}\n",
    "\n",
    "print(ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_polygon_dict(polygon_dict: dict) -> tuple:\n",
    "    gdf = gpd.GeoDataFrame.from_features([polygon_dict])\n",
    "    polygon = gdf.geometry.iloc[0]\n",
    "    bounds = polygon.bounds\n",
    "    return bounds\n",
    "\n",
    "try:\n",
    "    bbox = get_bounds_polygon_dict(ROI)\n",
    "    print(bbox)\n",
    "except:\n",
    "    print(\"Draw a polygon using the rectangle tool\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your remaining filter parameters and perform the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a start and end date\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2023-06-01\"\n",
    "max_clouds = 30 # the maximum cloud cover percentage. Note this is over the WHOLE image, not just the ROI\n",
    "\n",
    "query = catalog.search(\n",
    "    collections=[COLLECTION], \n",
    "    datetime=f\"{start_date}/{end_date}\",\n",
    "    bbox=bbox,\n",
    "    query={\"eo:cloud_cover\":{\"lt\":max_clouds}},\n",
    ")\n",
    "\n",
    "items = list(query.get_items())\n",
    "print(f\"Found: {len(items):d} datasets\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get timerange of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dt = []\n",
    "for item in items:\n",
    "    dt = item.to_dict()['properties']['datetime']\n",
    "    dt = pd.to_datetime(dt) # .strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    all_dt.append(dt)\n",
    "\n",
    "all_dt = sorted(all_dt)\n",
    "min_date = min(all_dt)\n",
    "max_date = max(all_dt)\n",
    "\n",
    "min_date_str = min_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "max_date_str = max_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "date_range = f\"{min_date_str} to {max_date_str}\"\n",
    "print(date_range)\n",
    "\n",
    "time_window = max_date - min_date\n",
    "print(f\"Time window: {time_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_dt, columns=['Datetime'])\n",
    "\n",
    "# Extract date and time\n",
    "df['Date'] = df['Datetime'].dt.date\n",
    "df['Time'] = df['Datetime'].dt.time\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6, 4))\n",
    "\n",
    "# Date distribution (Histogram)\n",
    "sns.histplot(df['Date'], ax=ax[0])\n",
    "ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax[0].xaxis.set_major_locator(mdates.DayLocator(interval=5))  # adjust as needed\n",
    "plt.setp(ax[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "ax[0].set_title('Date Distribution')\n",
    "\n",
    "# Time distribution (Histogram)\n",
    "df['Time'].apply(lambda x: x.hour).plot(kind='hist', rwidth=0.9, ax=ax[1])\n",
    "ax[1].set_title('Hour Distribution')\n",
    "ax[1].set_xlabel('Hour of the day')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `stac_load` to retrieve an [xarray](https://docs.xarray.dev/en/stable/) dataset using the `items` returned by the STAC query\n",
    "\n",
    "We use dask to lazy load the data - view the dask dashboard to monitor processing. Note that running this cell multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stac_load(\n",
    "    items,\n",
    "    bands=(\"red\", \"green\", \"blue\"),\n",
    "    crs=\"epsg:3857\", # since resolution is in metres, we need to use a projected CRS\n",
    "    resolution=5, # the reoslution of the output image in metres\n",
    "    chunks={},  # <-- use Dask\n",
    "    groupby=\"id\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded lazily - we call `.compute()` to load the data, this can take minutes if there are many images to load or the bbox is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = dataset.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RGB image for each timestamp in the `dataset` using [Pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "\n",
    "Each image is written to the `OUTPUTS_DIR`\n",
    "\n",
    "Note that conversion to uint8 will quantize the data in order to reduce the filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def normalise_img(x):\n",
    "    \"Scale pixel values to 0-1 range\"\n",
    "    return x / x.max()\n",
    "\n",
    "for i in range(len(dataset.time)):\n",
    "    ds = dataset.isel(time=i)\n",
    "    dataarray = ds.to_array(\"band\")\n",
    "    date = dataarray.time.values\n",
    "    date = pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    rgb_numpy_array = np.transpose(dataarray.values, (1, 2, 0))\n",
    "    rgb_numpy_array = normalise_img(rgb_numpy_array)\n",
    "    Image.fromarray((rgb_numpy_array * 255).astype(np.uint8)).save(OUTPUTS_DIR + f\"{SITE}_{date}.png\")\n",
    "\n",
    "rgb_images = sorted(glob(OUTPUTS_DIR + \"*.png\"))\n",
    "len(rgb_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = rgb_images[0]\n",
    "print(fname)\n",
    "img = Image.open(fname)\n",
    "img.thumbnail((2200, 2200)) # resizes image in-place for consistent viewing\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a GIF of all frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add the filename text to the image\n",
    "def add_filename_to_image(image, filename, font):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    text_position = (10, 10)\n",
    "    text_color = (255, 255, 255)\n",
    "    draw.text(text_position, filename, font=font, fill=text_color)\n",
    "    return image\n",
    "\n",
    "def create_gif(input_folder, output_gif, duration=100):\n",
    "    png_files = [f for f in os.listdir(input_folder) if f.endswith(\".png\")]\n",
    "    png_files = sorted(png_files)\n",
    "    first_image = Image.open(os.path.join(input_folder, png_files[0])).convert(\"RGB\") # Load the first image and convert it to RGB mode\n",
    "    images = [] # Create a list to store the rest of the images\n",
    "    font = ImageFont.load_default() # Create a font object for writing the filename\n",
    "    first_image = add_filename_to_image(first_image, png_files[0], font)\n",
    "\n",
    "    # Iterate through the rest of the files, open them, convert to RGB mode, and append to the images list\n",
    "    for file in png_files[1:]:\n",
    "        image = Image.open(os.path.join(input_folder, file)).convert(\"RGB\")\n",
    "        file_id = file.split(\".\")[0]\n",
    "        image_with_filename = add_filename_to_image(image, file_id, font)\n",
    "        images.append(image_with_filename)\n",
    "\n",
    "    # Save the GIF using the save method from the first image\n",
    "    first_image.save(\n",
    "        output_gif,\n",
    "        save_all=True,\n",
    "        append_images=images,\n",
    "        duration=duration,\n",
    "        loop=0,\n",
    "        optimize=True,\n",
    "    )\n",
    "    print(f\"Wrote {output_gif}\")\n",
    "\n",
    "DURATION = len(rgb_images) * 2\n",
    "create_gif(OUTPUTS_DIR, f\"{OUTPUTS_DIR}{SITE}.gif\", duration=DURATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the generated GIF in Chrome to view it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
