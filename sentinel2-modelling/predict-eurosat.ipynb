{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chip Classification using EuroSAT - Predict\n",
    "\n",
    "This notebook demonstrates prediction using a chip classifier trained in `train-eurosat` on a Sentinel 2 dataset called [EuroSAT](https://github.com/phelber/EuroSAT). Note that using the [wandb logger](https://wandb.ai/) only requires a free account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup \n",
    "\n",
    "Refer to README.md for environment setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If using LightningAI, change the current working directory to the directory containing this notebook. \n",
    "REPO_DIR = \"/teamspace/studios/this_studio/eda-bids-hackathon-prep/\"  # Adjust as appropriate\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(os.path.join(REPO_DIR, \"sentinel2-modelling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Dict, Optional, Any\n",
    "from typing import Callable, Optional, cast\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchgeo.datasets import EuroSAT\n",
    "from torchgeo.datamodules import EuroSATDataModule\n",
    "from torchgeo.transforms import AugmentationSequential, indices\n",
    "from torchgeo.trainers import ClassificationTask\n",
    "from torchgeo.models import ResNet18_Weights\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "seed_everything(543)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Load EDS credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = ('B04', 'B03', 'B02') # make sure these match the model trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Having trained a model from the `train-eurosat` notebook, we will now predict with it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    batch_size = 128\n",
    "    num_workers = 8\n",
    "elif device ==  \"cpu\":\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "else:\n",
    "    print(\"unknown device!\")\n",
    "\n",
    "datamodule = EuroSATDataModule(\n",
    "    batch_size=batch_size, \n",
    "    root=\"data\", \n",
    "    num_workers=num_workers,\n",
    "    bands=BANDS,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a model checkpoint from wandb or point to a local checkpoint - note an [issue](https://github.com/microsoft/torchgeo/issues/1639) with the SENTINEL2_ALL_MOCO & RGB weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/teamspace/studios/this_studio/wandb_logs/eurosat/6knkh8o7/checkpoints/epoch=4-step=130.ckpt'\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ClassificationTask.load_from_checkpoint(ckpt_path, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup(stage=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # limit_predict_batches=1 # for a single batch only\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(model=task, dataloaders=datamodule) # "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference a single image\n",
    "Note that since the data was not normalised prior to training, it is possible to pass through an image without normalisation, but we will do so to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample = datamodule.test_dataset[2500]\n",
    "label = cast(int, sample[\"label\"].item())\n",
    "image = sample['image'].unsqueeze(0).to(device)\n",
    "pred = task(image)\n",
    "pred_index = int(torch.argmax(pred))\n",
    "\n",
    "result_str = f\"label: {datamodule.test_dataset.classes[label]}, prediction: {datamodule.test_dataset.classes[pred_index]}\"\n",
    "fig = datamodule.test_dataset.plot(sample, suptitle=result_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "task.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in datamodule.test_dataloader():\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        preds = task(images)\n",
    "        preds_indices = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred) # , labels=datamodule.test_dataset.classes\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(cm, datamodule.test_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
